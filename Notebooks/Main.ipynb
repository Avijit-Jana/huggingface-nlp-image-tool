{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDDnQquiZEsl"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z1LPabgfowBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad3f73a-d99a-45ed-b5fe-1335caf27155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m268.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m190.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m163.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Essential installations with evaluation tools\n",
        "!pip install -q  --no-cache-dir transformers torch gradio accelerate safetensors pillow diffusers scikit-learn numpy pandas\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import gradio as gr\n",
        "from typing import Optional, List, Tuple, Any, Dict\n",
        "from transformers.pipelines import pipeline\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import time\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbSWPVsZEsm"
      },
      "source": [
        "# Model Pipeline Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NBiKw__kowBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f85c9de-06ec-4024-f2a5-1a36116b4c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ EvaluatedNLPTool initialized on device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configure environment\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "class EvaluatedNLPTool:\n",
        "    \"\"\"\n",
        "    NLP tool with comprehensive evaluation metrics based on project requirements\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.current_pipeline = None\n",
        "        self.current_task = None\n",
        "        self.conversation_history = []\n",
        "        self.evaluation_data = {}\n",
        "        self.image_pipe = None  # Initialize image pipeline attribute\n",
        "        self.task_feedback = {}  # Store feedback for each task\n",
        "\n",
        "        # Model configurations\n",
        "        self.model_configs = {\n",
        "            \"summarization\": {\n",
        "                \"model_name\": \"facebook/bart-large-cnn\",\n",
        "                \"task\": \"summarization\"\n",
        "            },\n",
        "            \"sentiment\": {\n",
        "                \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "                \"task\": \"sentiment-analysis\"\n",
        "            },\n",
        "            \"qa\": {\n",
        "                \"model_name\": \"deepset/roberta-base-squad2\",\n",
        "                \"task\": \"question-answering\"\n",
        "            },\n",
        "            \"next_word\": {\n",
        "                \"model_name\": \"gpt2-medium\",\n",
        "                \"task\": \"text-generation\"\n",
        "            },\n",
        "            \"text_generation\": {\n",
        "                \"model_name\": \"gpt2-large\",  # Enhanced for story generation\n",
        "                \"task\": \"text-generation\"\n",
        "            },\n",
        "            \"chatbot\": {\n",
        "                \"model_name\": \"gpt2-large\",  # Upgraded to GPT-2 Large for better responses\n",
        "                \"task\": \"text-generation\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"üöÄ EvaluatedNLPTool initialized on device: {self.device}\")\n",
        "        self._init_evaluation_storage()\n",
        "\n",
        "    def _init_evaluation_storage(self):\n",
        "        \"\"\"Initialize evaluation metrics storage\"\"\"\n",
        "        self.evaluation_data = {\n",
        "            \"accuracy\": [],\n",
        "            \"precision\": [],\n",
        "            \"recall\": [],\n",
        "            \"f1_score\": [],\n",
        "            \"user_satisfaction\": [],\n",
        "            \"response_times\": [],\n",
        "            \"task_performance\": {},\n",
        "            \"task_feedback_history\": []\n",
        "        }\n",
        "\n",
        "        # Initialize task-specific feedback storage\n",
        "        self.task_feedback = {\n",
        "            \"summarization\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"sentiment\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"qa\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"next_word\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"story_generation\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"chatbot\": {\"ratings\": [], \"response_times\": []},\n",
        "            \"image_generation\": {\"ratings\": [], \"response_times\": []}\n",
        "        }\n",
        "\n",
        "    def clear_gpu_memory(self):\n",
        "        \"\"\"Enhanced GPU memory cleanup\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "        gc.collect()\n",
        "        print(\"üßπ GPU memory cleared\")\n",
        "\n",
        "    def aggressive_memory_cleanup(self):\n",
        "        \"\"\"Aggressive memory cleanup for image generation\"\"\"\n",
        "        if hasattr(self, 'image_pipe') and self.image_pipe is not None:\n",
        "            del self.image_pipe\n",
        "            self.image_pipe = None\n",
        "\n",
        "        if self.current_pipeline is not None:\n",
        "            del self.current_pipeline\n",
        "            self.current_pipeline = None\n",
        "            self.current_task = None\n",
        "\n",
        "        self.clear_gpu_memory()\n",
        "        time.sleep(2)  # Allow memory to be freed\n",
        "        print(\"üö® Aggressive memory cleanup completed\")\n",
        "\n",
        "    def load_pipeline_efficiently(self, task: str) -> bool:\n",
        "        \"\"\"Smart pipeline loading with enhanced memory management\"\"\"\n",
        "        if self.current_task == task and self.current_pipeline:\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            config = self.model_configs.get(task)\n",
        "            if not config:\n",
        "                return False\n",
        "\n",
        "            if self.current_pipeline:\n",
        "                del self.current_pipeline\n",
        "                self.clear_gpu_memory()\n",
        "                time.sleep(1)\n",
        "\n",
        "            print(f\"üì• Loading {task} model...\")\n",
        "\n",
        "            self.current_pipeline = pipeline(\n",
        "                config[\"task\"],\n",
        "                model=config[\"model_name\"],\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "            )\n",
        "\n",
        "            self.current_task = task\n",
        "            print(f\"‚úÖ Successfully loaded {task} model\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {task} model: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def record_task_feedback(self, task_name: str, rating: int, response_time: float):\n",
        "        \"\"\"Record user feedback for specific tasks\"\"\"\n",
        "        if task_name in self.task_feedback:\n",
        "            self.task_feedback[task_name][\"ratings\"].append(rating)\n",
        "            self.task_feedback[task_name][\"response_times\"].append(response_time)\n",
        "\n",
        "            # Also add to general evaluation data\n",
        "            self.evaluation_data[\"user_satisfaction\"].append(rating)\n",
        "            self.evaluation_data[\"response_times\"].append(response_time)\n",
        "\n",
        "            # Store detailed feedback history\n",
        "            feedback_entry = {\n",
        "                \"task\": task_name,\n",
        "                \"rating\": rating,\n",
        "                \"response_time\": response_time,\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "            self.evaluation_data[\"task_feedback_history\"].append(feedback_entry)\n",
        "\n",
        "    def text_summarization(self, text: str, max_length: int = 150) -> Tuple[str, float]:\n",
        "        \"\"\"Clean summarization output\"\"\"\n",
        "        if not text.strip():\n",
        "            return \"Please provide text to summarize.\", 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"summarization\"):\n",
        "                return \"Failed to load summarization model.\", 0.0\n",
        "\n",
        "            result = self.current_pipeline(\n",
        "                text[:1000],\n",
        "                max_length=max_length,\n",
        "                min_length=30,\n",
        "                do_sample=False,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            self.evaluation_data[\"response_times\"].append(response_time)\n",
        "\n",
        "            return result[0]['summary_text'], response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error in summarization: {str(e)}\", 0.0\n",
        "\n",
        "    def sentiment_analysis(self, text: str) -> Tuple[str, float]:\n",
        "        \"\"\"Clean sentiment analysis output\"\"\"\n",
        "        if not text.strip():\n",
        "            return \"Please provide text for sentiment analysis.\", 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"sentiment\"):\n",
        "                return \"Failed to load sentiment model.\", 0.0\n",
        "\n",
        "            result = self.current_pipeline(text[:512])\n",
        "            response_time = time.time() - start_time\n",
        "            self.evaluation_data[\"response_times\"].append(response_time)\n",
        "\n",
        "            label = result[0]['label']\n",
        "            score = result[0]['score']\n",
        "\n",
        "            sentiment_map = {\n",
        "                'LABEL_0': 'Negative',\n",
        "                'LABEL_1': 'Neutral',\n",
        "                'LABEL_2': 'Positive',\n",
        "                'NEGATIVE': 'Negative',\n",
        "                'POSITIVE': 'Positive'\n",
        "            }\n",
        "\n",
        "            sentiment = sentiment_map.get(label, label)\n",
        "            return f\"Sentiment: {sentiment}\\nConfidence: {score:.2%}\", response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error in sentiment analysis: {str(e)}\", 0.0\n",
        "\n",
        "    def question_answering(self, question: str, context: str) -> Tuple[str, float]:\n",
        "        \"\"\"Enhanced QA with timing\"\"\"\n",
        "        if not question.strip() or not context.strip():\n",
        "            return \"Please provide both question and context.\", 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"qa\"):\n",
        "                return \"Failed to load QA model.\", 0.0\n",
        "\n",
        "            result = self.current_pipeline(\n",
        "                question=question,\n",
        "                context=context[:800]\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            self.evaluation_data[\"response_times\"].append(response_time)\n",
        "            return f\"{result['answer']} (Confidence: {result['score']:.2%})\", response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error in question answering: {str(e)}\", 0.0\n",
        "\n",
        "    def next_word_prediction(self, prompt: str) -> Tuple[str, float]:\n",
        "        \"\"\"Next word prediction - predicts only 1-3 words\"\"\"\n",
        "        if not prompt.strip():\n",
        "            return \"Please provide a prompt for next word prediction.\", 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"next_word\"):\n",
        "                return \"Failed to load next word prediction model.\", 0.0\n",
        "\n",
        "            result = self.current_pipeline(\n",
        "                prompt[-100:],  # Use last 100 characters for context\n",
        "                max_new_tokens=3,  # Only predict 1-3 words\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                return_full_text=False  # Only return new tokens\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            predicted_text = result[0]['generated_text']\n",
        "\n",
        "            # Clean up the prediction\n",
        "            predicted_text = predicted_text.strip()\n",
        "            if not predicted_text:\n",
        "                predicted_text = \"[Unable to predict next word]\"\n",
        "\n",
        "            return predicted_text, response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error in next word prediction: {str(e)}\", 0.0\n",
        "\n",
        "    def story_generation(self, prompt: str, max_length: int = 150) -> Tuple[str, float]:\n",
        "        \"\"\"Enhanced story generation with better model\"\"\"\n",
        "        if not prompt.strip():\n",
        "            return \"Please provide a story prompt.\", 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"text_generation\"):\n",
        "                return \"Failed to load story generation model.\", 0.0\n",
        "\n",
        "            result = self.current_pipeline(\n",
        "                prompt[:200],\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            generated_story = result[0]['generated_text']\n",
        "\n",
        "            # Clean up the story\n",
        "            if generated_story.startswith(prompt):\n",
        "                generated_story = generated_story[len(prompt):].strip()\n",
        "\n",
        "            return generated_story, response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error in story generation: {str(e)}\", 0.0\n",
        "\n",
        "    def chatbot_response(self, message: str, history: List) -> Tuple[str, List, float]:\n",
        "        \"\"\"Enhanced chatbot with GPT-2 Large\"\"\"\n",
        "        if not message.strip():\n",
        "            return \"\", history, 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            if not self.load_pipeline_efficiently(\"chatbot\"):\n",
        "                error_response = \"Failed to load chatbot model.\"\n",
        "                history.append([message, error_response])\n",
        "                return \"\", history, 0.0\n",
        "\n",
        "            # Build conversation context more intelligently\n",
        "            context = \"\"\n",
        "            if history:\n",
        "                # Use last 2 exchanges for context to avoid token limit\n",
        "                recent_history = history[-2:]\n",
        "                for user_msg, bot_msg in recent_history:\n",
        "                    context += f\"User: {user_msg}\\nBot: {bot_msg}\\n\"\n",
        "\n",
        "            # Enhanced prompt engineering\n",
        "            full_prompt = f\"{context}User: {message}\\nBot:\"\n",
        "\n",
        "            # Generate response with better parameters\n",
        "            result = self.current_pipeline(\n",
        "                full_prompt,\n",
        "                max_new_tokens=30,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256,\n",
        "                eos_token_id=50256,\n",
        "                repetition_penalty=1.2,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            response = result[0]['generated_text']\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            # Extract bot response more intelligently\n",
        "            if \"Bot:\" in response:\n",
        "                bot_response = response.split(\"Bot:\")[-1].strip()\n",
        "                # Clean up response\n",
        "                if \"User:\" in bot_response:\n",
        "                    bot_response = bot_response.split(\"User:\")[0].strip()\n",
        "            else:\n",
        "                bot_response = \"I'm here to help! Could you please rephrase your question?\"\n",
        "\n",
        "            # Final cleanup\n",
        "            bot_response = bot_response.replace(\"User:\", \"\").replace(\"Bot:\", \"\").strip()\n",
        "\n",
        "            # Ensure reasonable length\n",
        "            if len(bot_response) > 200:\n",
        "                bot_response = bot_response[:200] + \"...\"\n",
        "\n",
        "            if not bot_response or len(bot_response) < 3:\n",
        "                bot_response = \"I understand. Please tell me more about what you'd like to know!\"\n",
        "\n",
        "            history.append([message, bot_response])\n",
        "            return \"\", history, response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any exceptions during chatbot response generation\n",
        "            error_response = f\"Error in chatbot response: {str(e)}\"\n",
        "            history.append([message, error_response])\n",
        "            return \"\", history, 0.0\n",
        "\n",
        "    def generate_image(self, prompt: str, steps: int = 20) -> tuple:\n",
        "        \"\"\"Actual image generation\"\"\"\n",
        "        if not prompt.strip():\n",
        "            return None, \"Please provide a prompt for image generation.\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            # Aggressive cleanup before loading image model\n",
        "            if self.current_pipeline is not None:\n",
        "                print(\"üßπ Cleaning up NLP models for image generation...\")\n",
        "                del self.current_pipeline\n",
        "                self.current_pipeline = None\n",
        "                self.current_task = None\n",
        "                self.clear_gpu_memory()\n",
        "\n",
        "            if not hasattr(self, 'image_pipe') or self.image_pipe is None:\n",
        "                print(\"üé® Loading image generation model...\")\n",
        "                self.image_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                    \"runwayml/stable-diffusion-v1-5\",\n",
        "                    torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                    use_safetensors=True\n",
        "                )\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    self.image_pipe.enable_attention_slicing()\n",
        "                    self.image_pipe.enable_sequential_cpu_offload()\n",
        "\n",
        "            image = self.image_pipe(\n",
        "                prompt,\n",
        "                num_inference_steps=steps,\n",
        "                width=512,\n",
        "                height=512,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            # Immediate cleanup after generation\n",
        "            self.clear_gpu_memory()\n",
        "            return image, f\"‚úÖ Generated image: '{prompt}' in {response_time:.2f}s\", response_time\n",
        "\n",
        "        except Exception as e:\n",
        "            # Emergency cleanup on error\n",
        "            self.aggressive_memory_cleanup()\n",
        "            return None, f\"‚ùå Image generation error: {str(e)}\", 0.0\n",
        "\n",
        "    def get_evaluation_report(self) -> str:\n",
        "        \"\"\"Enhanced evaluation report with task-specific metrics\"\"\"\n",
        "        try:\n",
        "            if not any(self.task_feedback.values()):\n",
        "                return \"No evaluation data available yet. Use the tools to generate metrics.\"\n",
        "\n",
        "            report = \"# üìä Comprehensive Model Evaluation Report\\n\\n\"\n",
        "\n",
        "            # Overall Statistics\n",
        "            total_tasks = sum(len(data[\"ratings\"]) for data in self.task_feedback.values())\n",
        "            if total_tasks > 0:\n",
        "                overall_satisfaction = np.mean([rating for data in self.task_feedback.values() for rating in data[\"ratings\"]])\n",
        "                overall_response_time = np.mean([time for data in self.task_feedback.values() for time in data[\"response_times\"]])\n",
        "\n",
        "                report += f\"## üéØ Overall Performance\\n\"\n",
        "                report += f\"- **Total Tasks Completed**: {total_tasks}\\n\"\n",
        "                report += f\"- **Overall Satisfaction**: {overall_satisfaction:.2f}/5.0 ‚≠ê\\n\"\n",
        "                report += f\"- **Average Response Time**: {overall_response_time:.2f} seconds ‚ö°\\n\\n\"\n",
        "\n",
        "            # Task-specific performance\n",
        "            report += \"## üìà Task-Specific Performance\\n\\n\"\n",
        "\n",
        "            for task_name, data in self.task_feedback.items():\n",
        "                if data[\"ratings\"]:\n",
        "                    avg_rating = np.mean(data[\"ratings\"])\n",
        "                    avg_time = np.mean(data[\"response_times\"])\n",
        "                    task_count = len(data[\"ratings\"])\n",
        "\n",
        "                    # Determine performance emoji\n",
        "                    if avg_rating >= 4.5:\n",
        "                        emoji = \"üî•\"\n",
        "                    elif avg_rating >= 4.0:\n",
        "                        emoji = \"üöÄ\"\n",
        "                    elif avg_rating >= 3.5:\n",
        "                        emoji = \"üëç\"\n",
        "                    elif avg_rating >= 3.0:\n",
        "                        emoji = \"üëå\"\n",
        "                    else:\n",
        "                        emoji = \"‚ö†Ô∏è\"\n",
        "\n",
        "                    report += f\"### {emoji} {task_name.replace('_', ' ').title()}\\n\"\n",
        "                    report += f\"- **Usage Count**: {task_count}\\n\"\n",
        "                    report += f\"- **Average Rating**: {avg_rating:.2f}/5.0\\n\"\n",
        "                    report += f\"- **Average Response Time**: {avg_time:.2f}s\\n\"\n",
        "                    report += f\"- **Performance Status**: {'Excellent' if avg_rating >= 4.5 else 'Good' if avg_rating >= 4.0 else 'Fair' if avg_rating >= 3.0 else 'Needs Improvement'}\\n\\n\"\n",
        "\n",
        "            # Recent feedback history\n",
        "            if self.evaluation_data[\"task_feedback_history\"]:\n",
        "                report += \"## üìù Recent Activity\\n\\n\"\n",
        "                recent_feedback = self.evaluation_data[\"task_feedback_history\"][-5:]  # Last 5 entries\n",
        "                for entry in recent_feedback:\n",
        "                    report += f\"- **{entry['task'].replace('_', ' ').title()}**: {entry['rating']}/5 ‚≠ê | {entry['response_time']:.2f}s | {entry['timestamp']}\\n\"\n",
        "\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating evaluation report: {str(e)}\"\n",
        "\n",
        "# Initialize the enhanced tool\n",
        "nlp_tool = EvaluatedNLPTool()\n",
        "\n",
        "def create_feedback_component():\n",
        "    \"\"\"Create reusable feedback component\"\"\"\n",
        "    with gr.Row(visible=False) as feedback_row:\n",
        "        gr.Markdown(\"### üìù Rate this response:\")\n",
        "        rating = gr.Slider(1, 5, value=5, label=\"Satisfaction Rating\", step=1)\n",
        "        submit_feedback = gr.Button(\"Submit Feedback\", variant=\"secondary\", size=\"sm\")\n",
        "        feedback_status = gr.Textbox(label=\"\", interactive=False, visible=False)\n",
        "\n",
        "    return feedback_row, rating, submit_feedback, feedback_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS3Lp0VYZEsn"
      },
      "source": [
        "# Gradio WebApp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_UWxOFaMowBP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "3dfd5acb-f300-4f87-a630-c62942a51da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://20abfa41af14ce1bb1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://20abfa41af14ce1bb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Enhanced NLP Tool with Universal Feedback System launched successfully!\n",
            "üì± Access your app using the shareable link above\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive Gradio interface\n",
        "def create_enhanced_interface():\n",
        "    \"\"\"Create feature-rich Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"ü§ñ Enhanced NLP Tool\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        <h1 align=\"center\"> ü§ñ Enhanced Multifunctional NLP & AI Tool</h1>\n",
        "\n",
        "        <p align=\"center\"><b> üî• Optimized for Google Colab with Smart Memory Management & User Feedback System <b></p>\n",
        "\n",
        "        ## üöÄ Enhanced Features:\n",
        "        - üìù **Text Summarization**: Intelligent content condensation with feedback\n",
        "        - üòä **Sentiment Analysis**: Emotional tone detection with confidence scoring\n",
        "        - ‚ùì **Question Answering**: Context-based intelligent responses\n",
        "        - üîÆ **Next Word Prediction**: Predicts 1-3 next words from your input\n",
        "        - üìö **Story Generation**: Creative storytelling with enhanced GPT-2 Large\n",
        "        - üí¨ **Intelligent Chatbot**: Powered by GPT-2 Large for better conversations\n",
        "        - üé® **Image Generation**: Text-to-image with aggressive memory management\n",
        "        - üìä **Advanced Analytics**: Task-specific performance tracking & user feedback\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "\n",
        "            # Enhanced Summarization Tab\n",
        "            with gr.TabItem(\"üìù Text Summarization\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        sum_input = gr.Textbox(\n",
        "                            lines=8,\n",
        "                            placeholder=\"Enter text to summarize (optimized for up to 1000 characters)...\",\n",
        "                            label=\"Input Text\",\n",
        "                            max_lines=12\n",
        "                        )\n",
        "                        sum_length = gr.Slider(\n",
        "                            50, 300, value=150,\n",
        "                            label=\"Summary Length\",\n",
        "                            info=\"Adjust summary detail level\"\n",
        "                        )\n",
        "                        sum_btn = gr.Button(\"üìù Generate Summary\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        sum_output = gr.Textbox(\n",
        "                            lines=8,\n",
        "                            label=\"Summary Output\",\n",
        "                            interactive=False,\n",
        "                            placeholder=\"Your intelligent summary will appear here...\"\n",
        "                        )\n",
        "                        sum_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "\n",
        "                # Feedback component for summarization\n",
        "                sum_feedback_row, sum_rating, sum_feedback_btn, sum_feedback_status = create_feedback_component()\n",
        "\n",
        "                def summarize_with_feedback(text, length):\n",
        "                    result, resp_time = nlp_tool.text_summarization(text, length)\n",
        "                    return result, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_sum_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"summarization\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                # Wire up the buttons\n",
        "                sum_btn.click(\n",
        "                    summarize_with_feedback,\n",
        "                    inputs=[sum_input, sum_length],\n",
        "                    outputs=[sum_output, sum_time, sum_feedback_row]\n",
        "                )\n",
        "                sum_feedback_btn.click(\n",
        "                    submit_sum_feedback,\n",
        "                    inputs=[sum_rating, sum_time],\n",
        "                    outputs=[sum_feedback_status, sum_feedback_row]\n",
        "                )\n",
        "\n",
        "                # Example inputs/hooks\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [\"Artificial intelligence has revolutionized numerous industries by automating complex processes, enhancing decision-making capabilities, and enabling unprecedented efficiency gains. Machine learning algorithms analyze vast datasets to identify patterns, predict outcomes, and optimize operations across healthcare, finance, transportation, and manufacturing sectors.\", 100],\n",
        "                        [\"Climate change represents one of humanity's greatest challenges, with rising global temperatures causing sea level increases, extreme weather events, ecosystem disruptions, and biodiversity loss. Mitigation strategies include renewable energy adoption, carbon capture technologies, sustainable agriculture practices, and international cooperation frameworks.\", 80]\n",
        "                    ],\n",
        "                    inputs=[sum_input, sum_length],\n",
        "                    outputs=sum_output,\n",
        "                    fn=nlp_tool.text_summarization\n",
        "                )\n",
        "\n",
        "            # Enhanced Sentiment Analysis Tab\n",
        "            with gr.TabItem(\"üòä Sentiment Analysis\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        sent_input = gr.Textbox(\n",
        "                            lines=5,\n",
        "                            placeholder=\"Enter text to analyze emotional tone...\",\n",
        "                            label=\"Input Text\"\n",
        "                        )\n",
        "                        sent_btn = gr.Button(\"üòä Analyze Sentiment\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        sent_output = gr.Textbox(\n",
        "                            lines=5,\n",
        "                            label=\"Sentiment Analysis Result\",\n",
        "                            interactive=False\n",
        "                        )\n",
        "                        sent_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "\n",
        "                sent_feedback_row, sent_rating, sent_feedback_btn, sent_feedback_status = create_feedback_component()\n",
        "\n",
        "                def analyze_sentiment_with_feedback(text):\n",
        "                    result, resp_time = nlp_tool.sentiment_analysis(text)\n",
        "                    return result, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_sent_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"sentiment\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                sent_btn.click(analyze_sentiment_with_feedback, sent_input,\n",
        "                              [sent_output, sent_time, sent_feedback_row])\n",
        "                sent_feedback_btn.click(submit_sent_feedback, [sent_rating, sent_time],\n",
        "                                       [sent_feedback_status, sent_feedback_row])\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        \"I absolutely love this new feature! It works perfectly and exceeded my expectations.\",\n",
        "                        \"This product is quite disappointing and doesn't meet the advertised specifications.\",\n",
        "                        \"The service is adequate, nothing exceptional but gets the job done reasonably well.\",\n",
        "                        \"I'm excited about the possibilities this technology brings to our workflow!\"\n",
        "                    ],\n",
        "                    inputs=sent_input,\n",
        "                    outputs=sent_output,\n",
        "                    fn=nlp_tool.sentiment_analysis\n",
        "                )\n",
        "\n",
        "            # Enhanced Question Answering Tab - FIXED\n",
        "            with gr.TabItem(\"‚ùì Question Answering\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        qa_question = gr.Textbox(\n",
        "                            lines=2,\n",
        "                            placeholder=\"Enter your question...\",\n",
        "                            label=\"Question\"\n",
        "                        )\n",
        "                        qa_context = gr.Textbox(\n",
        "                            lines=6,\n",
        "                            placeholder=\"Provide context containing the answer...\",\n",
        "                            label=\"Context\"\n",
        "                        )\n",
        "                        qa_btn = gr.Button(\"‚ùì Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        qa_output = gr.Textbox(\n",
        "                            lines=8,\n",
        "                            label=\"Answer with Confidence\",\n",
        "                            interactive=False\n",
        "                        )\n",
        "                        qa_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "\n",
        "                qa_feedback_row, qa_rating, qa_feedback_btn, qa_feedback_status = create_feedback_component()\n",
        "\n",
        "                def answer_question_with_feedback(question, context):\n",
        "                    result, resp_time = nlp_tool.question_answering(question, context)\n",
        "                    return result, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_qa_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"qa\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                qa_btn.click(answer_question_with_feedback, [qa_question, qa_context],\n",
        "                            [qa_output, qa_time, qa_feedback_row])\n",
        "                qa_feedback_btn.click(submit_qa_feedback, [qa_rating, qa_time],\n",
        "                                     [qa_feedback_status, qa_feedback_row])\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [\"What is machine learning?\", \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make predictions or decisions.\"],\n",
        "                        [\"When was the company founded?\", \"TechCorp was founded in 2010 by three university graduates who wanted to revolutionize cloud computing. The company started in a small garage and now employs over 5000 people worldwide.\"]\n",
        "                    ],\n",
        "                    inputs=[qa_question, qa_context],\n",
        "                    outputs=qa_output,\n",
        "                    fn=nlp_tool.question_answering\n",
        "                )\n",
        "\n",
        "            # Next Word Prediction Tab\n",
        "            with gr.TabItem(\"üîÆ Text Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        nw_input = gr.Textbox(\n",
        "                            lines=3,\n",
        "                            placeholder=\"Enter text and I'll predict the next 1-3 words...\",\n",
        "                            label=\"Text Prompt\"\n",
        "                        )\n",
        "                        nw_btn = gr.Button(\"üîÆ Predict Next Words\", variant=\"primary\", size=\"lg\") # Added nw_btn definition\n",
        "                    with gr.Column():\n",
        "                        nw_output = gr.Textbox(\n",
        "                            lines=3,\n",
        "                            label=\"Predicted Next Words\",\n",
        "                            interactive=False\n",
        "                        )\n",
        "                        nw_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "\n",
        "\n",
        "                nw_feedback_row, nw_rating, nw_feedback_btn, nw_feedback_status = create_feedback_component()\n",
        "\n",
        "                def predict_next_words_with_feedback(text):\n",
        "                    result, resp_time = nlp_tool.next_word_prediction(text)\n",
        "                    return result, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_nw_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"next_word\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                nw_btn.click(predict_next_words_with_feedback, nw_input,\n",
        "                            [nw_output, nw_time, nw_feedback_row])\n",
        "                nw_feedback_btn.click(submit_nw_feedback, [nw_rating, nw_time],\n",
        "                                     [nw_feedback_status, nw_feedback_row])\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        \"The weather today is\",\n",
        "                        \"Machine learning is a subset of\",\n",
        "                        \"In the future, artificial intelligence will\"\n",
        "                    ],\n",
        "                    inputs=nw_input,\n",
        "                    outputs=nw_output,\n",
        "                    fn=lambda x: nlp_tool.next_word_prediction(x)[0]\n",
        "                )\n",
        "\n",
        "            # Story Generation Tab\n",
        "            with gr.TabItem(\"üìö Story Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        story_input = gr.Textbox(\n",
        "                            lines=3,\n",
        "                            placeholder=\"Enter a story prompt or beginning...\",\n",
        "                            label=\"Story Prompt\"\n",
        "                        )\n",
        "                        story_length = gr.Slider(\n",
        "                            50, 300, value=150,\n",
        "                            label=\"Story Length\",\n",
        "                            info=\"Longer stories for more detail\"\n",
        "                        )\n",
        "                        story_btn = gr.Button(\"üìö Generate Story\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        story_output = gr.Textbox(\n",
        "                            lines=15,  # Increased height\n",
        "                            label=\"Generated Story\",\n",
        "                            interactive=False,\n",
        "                            max_lines=20,  # Enable scrolling\n",
        "                            show_copy_button=True\n",
        "                        )\n",
        "                        story_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "\n",
        "                story_feedback_row, story_rating, story_feedback_btn, story_feedback_status = create_feedback_component()\n",
        "\n",
        "                def generate_story_with_feedback(prompt, length):\n",
        "                    result, resp_time = nlp_tool.story_generation(prompt, length)\n",
        "                    return result, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_story_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"story_generation\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                story_btn.click(generate_story_with_feedback, [story_input, story_length],\n",
        "                               [story_output, story_time, story_feedback_row])\n",
        "                story_feedback_btn.click(submit_story_feedback, [story_rating, story_time],\n",
        "                                        [story_feedback_status, story_feedback_row])\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [\"Once upon a time in a digital realm\", 150],\n",
        "                        [\"The last data scientist on Earth discovered\", 180],\n",
        "                        [\"In the year 2050, AI and humans\", 160]\n",
        "                    ],\n",
        "                    inputs=[story_input, story_length],\n",
        "                    outputs=story_output,\n",
        "                    fn=lambda prompt, length: nlp_tool.story_generation(prompt, length)[0]\n",
        "                )\n",
        "\n",
        "            # Enhanced Chatbot Tab\n",
        "            with gr.TabItem(\"üí¨ Interactive Chatbot\"):\n",
        "                gr.Markdown(\"### Have an intelligent conversation with AI\")\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    height=450,\n",
        "                    label=\"AI Assistant Conversation\",\n",
        "                    show_label=True,\n",
        "                    avatar_images=(None, \"ü§ñ\")\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg = gr.Textbox(\n",
        "                        placeholder=\"Type your message here...\",\n",
        "                        label=\"Your Message\",\n",
        "                        lines=2,\n",
        "                        scale=4\n",
        "                    )\n",
        "                    send_btn = gr.Button(\"Send üì§\", variant=\"primary\", scale=1)\n",
        "\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear Conversation\", variant=\"secondary\")\n",
        "                chat_time = gr.State(0.0)\n",
        "\n",
        "                def send_message_with_timing(message, history):\n",
        "                    _, new_history, resp_time = nlp_tool.chatbot_response(message, history)\n",
        "                    return \"\", new_history, resp_time\n",
        "\n",
        "                msg.submit(send_message_with_timing, [msg, chatbot], [msg, chatbot, chat_time])\n",
        "                send_btn.click(send_message_with_timing, [msg, chatbot], [msg, chatbot, chat_time])\n",
        "                clear_btn.click(lambda: ([], \"\"), None, [chatbot, msg])\n",
        "\n",
        "            # Image Generation Tab\n",
        "            with gr.TabItem(\"üé® Image Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        img_input = gr.Textbox(\n",
        "                            lines=3,\n",
        "                            placeholder=\"Describe the image you want to create...\",\n",
        "                            label=\"Image Prompt\"\n",
        "                        )\n",
        "                        img_steps = gr.Slider(\n",
        "                            10, 50, value=20,\n",
        "                            label=\"Inference Steps\",\n",
        "                            info=\"More steps = better quality, slower generation\"\n",
        "                        )\n",
        "                        img_btn = gr.Button(\"üé® Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "                        memory_btn = gr.Button(\"üßπ Clear Memory\", variant=\"secondary\")\n",
        "\n",
        "                    with gr.Column():\n",
        "                        img_output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
        "                        img_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                        img_time = gr.Textbox(label=\"Response Time\", interactive=False, visible=False)\n",
        "                img_feedback_row, img_rating, img_feedback_btn, img_feedback_status = create_feedback_component()\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [\"A futuristic cityscape with flying cars and neon lights\", 20],\n",
        "                        [\"An abstract representation of data flowing through neural networks\", 25],\n",
        "                        [\"A serene landscape with mountains and a crystal-clear lake\", 30]\n",
        "                    ],\n",
        "                    inputs=[img_input, img_steps],\n",
        "                    outputs=[img_output, img_status],\n",
        "                )\n",
        "                def generate_image_with_feedback(prompt, steps):\n",
        "                    image, status, resp_time = nlp_tool.generate_image(prompt, steps)\n",
        "                    return image, status, f\"{resp_time:.2f}s\", gr.Row.update(visible=True)\n",
        "\n",
        "                def submit_img_feedback(rating, resp_time_str):\n",
        "                    try:\n",
        "                        resp_time = float(resp_time_str.replace('s', ''))\n",
        "                        nlp_tool.record_task_feedback(\"image_generation\", int(rating), resp_time)\n",
        "                        return \"‚úÖ Thank you for your feedback!\", gr.Row.update(visible=False)\n",
        "                    except:\n",
        "                        return \"‚ùå Error submitting feedback\", gr.Row.update(visible=False)\n",
        "\n",
        "                def manual_memory_clear():\n",
        "                    nlp_tool.aggressive_memory_cleanup()\n",
        "                    return \"üßπ Memory cleared successfully!\"\n",
        "\n",
        "                img_btn.click(generate_image_with_feedback, [img_input, img_steps],\n",
        "                             [img_output, img_status, img_time, img_feedback_row])\n",
        "                img_feedback_btn.click(submit_img_feedback, [img_rating, img_time],\n",
        "                                      [img_feedback_status, img_feedback_row])\n",
        "                memory_btn.click(manual_memory_clear, outputs=img_status)\n",
        "\n",
        "            # Enhanced Evaluation Report Tab\n",
        "            with gr.TabItem(\"üìä Advanced Analytics\"):\n",
        "                gr.Markdown(\"### üìà Comprehensive Performance Analytics & User Feedback\")\n",
        "\n",
        "                eval_btn = gr.Button(\"üìä Generate Detailed Report\", variant=\"primary\", size=\"lg\")\n",
        "                eval_output = gr.Markdown(\n",
        "                    value=\"Click the button above to generate your comprehensive evaluation report with task-specific metrics.\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    refresh_btn = gr.Button(\"üîÑ Refresh Data\", variant=\"secondary\")\n",
        "\n",
        "                eval_btn.click(nlp_tool.get_evaluation_report, outputs=eval_output)\n",
        "                refresh_btn.click(nlp_tool.get_evaluation_report, outputs=eval_output)\n",
        "\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the enhanced application\n",
        "demo = create_enhanced_interface()\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    debug=False,\n",
        "    show_error=True,\n",
        "    inbrowser=True\n",
        ")\n",
        "\n",
        "print(\"üéâ Enhanced NLP Tool with Universal Feedback System launched successfully!\")\n",
        "print(\"üì± Access your app using the shareable link above\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DDDnQquiZEsl",
        "ZS3Lp0VYZEsn"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}